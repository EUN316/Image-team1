{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KoBERT_encoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMXU6Z4endBlm7cbCw0kXQp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"E6Q-UwRkU3W5"},"source":["### KoBERT\n","\n","출처: https://github.com/SKTBrain/KoBERT\n","\n","https://colab.research.google.com/github/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb#scrollTo=FW-hpivF5m7Z\n","\n","- 공개된 코드를 인코더 형식에 맞게 재가공했습니다. multi 인코더나 gpt와 합친 버전 등 앞으로 구현할 방향에 맞게 계속해서 수정하시면 됩니다. "]},{"cell_type":"code","metadata":{"id":"lYobxV1bUlm-","executionInfo":{"status":"ok","timestamp":1604592076816,"user_tz":-540,"elapsed":325481,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}},"outputId":"071b9709-86da-4de1-9876-cf1a7a526b6c","colab":{"base_uri":"https://localhost:8080/"}},"source":["# !pip uninstall mxnet-cu101\n","!pip install mxnet-cu101\n","# !pip uninstall mxnet\n","# !pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece==0.1.85\n","!pip install transformers==2.1.1\n","!pip install torch==1.3.1\n","!pip install torchvision\n","!pip install kss\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting mxnet-cu101\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/26/9655677b901537f367c3c473376e4106abc72e01a8fc25b1cb6ed9c37e8c/mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0MB)\n","\u001b[K     |███████████████████████████████▌| 834.1MB 1.1MB/s eta 0:00:11tcmalloc: large alloc 1147494400 bytes == 0x396da000 @  0x7f907890e615 0x591e47 0x4cc179 0x4cc2db 0x50a1cc 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x58e683 0x50c127 0x58e683 0x50c127 0x58e683 0x50c127 0x58e683 0x50c127 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n","\u001b[K     |████████████████████████████████| 846.0MB 20kB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (2.23.0)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (1.18.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.6.20)\n","Installing collected packages: graphviz, mxnet-cu101\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-cu101-1.7.0\n","Collecting gluonnlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n","\u001b[K     |████████████████████████████████| 348kB 4.8MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.18.5)\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588521 sha256=325534318b8df4923d9f0eac7d89e222e203fe1f81fb9b7444fd89e378d8f023\n","  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Collecting sentencepiece==0.1.85\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 8.0MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.85\n","Collecting transformers==2.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n","\u001b[K     |████████████████████████████████| 317kB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2.23.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.1.85)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/56/7a7f901084210f4f12ea4c57f6265e4e690147ffaac6f68fb0a826403919/boto3-1.16.11-py2.py3-none-any.whl (129kB)\n","\u001b[K     |████████████████████████████████| 133kB 16.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.18.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 21.0MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (4.41.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (1.24.3)\n","Collecting botocore<1.20.0,>=1.19.11\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/b9/869df2dca1a9b874cf6c826a56bd2c7ff97770d66103876f99ec77b2c86d/botocore-1.19.11-py2.py3-none-any.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 30.9MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 8.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (0.17.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.11->boto3->transformers==2.1.1) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=fae8ba5b2a66c7c14300d10e72ce7f3475a1e888557b3635eca4b03390524c41\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","\u001b[31mERROR: botocore 1.19.11 has requirement urllib3<1.26,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, sacremoses, transformers\n","Successfully installed boto3-1.16.11 botocore-1.19.11 jmespath-0.10.0 s3transfer-0.3.3 sacremoses-0.0.43 transformers-2.1.1\n","Collecting torch==1.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n","\u001b[K     |████████████████████████████████| 734.6MB 15kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.18.5)\n","\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.3.1 which is incompatible.\u001b[0m\n","Installing collected packages: torch\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","Successfully installed torch-1.3.1\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n","Collecting torch==1.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/2a/58f8078744e0408619c63148f7a2e8e48cf007e4146b74d4bb67c56d161b/torch-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (776.7MB)\n","\u001b[K     |████████████████████████████████| 776.7MB 21kB/s \n","\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.16.0)\n","Installing collected packages: torch\n","  Found existing installation: torch 1.3.1\n","    Uninstalling torch-1.3.1:\n","      Successfully uninstalled torch-1.3.1\n","Successfully installed torch-1.7.0\n","Collecting kss\n","  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n","Building wheels for collected packages: kss\n","  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251563 sha256=3ee6192fa0d98de38201b49b4060d65339eccbaea97f5e1548a323097895f652\n","  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n","Successfully built kss\n","Installing collected packages: kss\n","Successfully installed kss-1.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NY_cX5ijUyy9","executionInfo":{"status":"ok","timestamp":1604592109115,"user_tz":-540,"elapsed":6362,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}},"outputId":"260b7ae4-c046-4a14-e6d0-bcc692f01531","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-qc6n7mwd\n","  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-qc6n7mwd\n","Building wheels for collected packages: kobert\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.1.1-cp36-none-any.whl size=12825 sha256=d9c56b56a40356c3d13889912032ee7fc0a5c46bbbb04486f6625caa1ecf947c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-u_5ijzzo/wheels/a2/b0/41/435ee4e918f91918be41529283c5ff86cd010f02e7525aecf3\n","Successfully built kobert\n","Installing collected packages: kobert\n","Successfully installed kobert-0.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9P88Lae5U7Ll","executionInfo":{"status":"ok","timestamp":1604592120685,"user_tz":-540,"elapsed":8641,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}}},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","from tqdm import tqdm, tqdm_notebook\n","import pandas as pd\n","import numpy as np\n","import time\n","import kss\n","import datetime"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"PXGe0bDfU-Pk","executionInfo":{"status":"ok","timestamp":1604592136265,"user_tz":-540,"elapsed":7321,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}}},"source":["from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMDjYw8UVFjO","executionInfo":{"status":"ok","timestamp":1604592144029,"user_tz":-540,"elapsed":927,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}}},"source":["##GPU 사용 시 #학습시에만 on\n","#device = torch.device(\"cuda:0\")\n","\n","##CPU\n","device = torch.device(\"cpu\")"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"T07hyvZrKhuu","executionInfo":{"status":"ok","timestamp":1604592531500,"user_tz":-540,"elapsed":55076,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}},"outputId":"3f29405d-fe61-4058-e18b-e8cd726eebf0","colab":{"base_uri":"https://localhost:8080/"}},"source":["bertmodel, vocab = get_pytorch_kobert_model()\n","\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["[██████████████████████████████████████████████████]\n","using cached model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DXjkRsE6CPgJ","executionInfo":{"status":"ok","timestamp":1604595265428,"user_tz":-540,"elapsed":998,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}}},"source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"17nhP27g6CY9","executionInfo":{"status":"ok","timestamp":1604595265751,"user_tz":-540,"elapsed":832,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}}},"source":["class EncoderBERT(nn.Module):\n","    def __init__(self, bert, hidden_size=768, params=None): #hidden_size: encoder output 차원 결정(오리지널 버전에 768로 고정되어있어서 바꾸고 싶으면..대공사를 해야하니 연락주셔요)\n","        super(EncoderBERT, self).__init__()\n","        self.bert = bert\n","\n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        #attention_mask, token_ids, segment_ids 등은 모두 bert 공부하시면 나오는 개념들이에요. 대부분 사이즈는 (nums of sentences, max_len)로 구성됩니다.\n","        #모르겠을땐 하나하나 print() 걸어줘서 직접 찍어보는게 이해하는데 제일 빠릅니다.\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length) #fine-tuning때 필요한거라 안쓰일듯\n","        sequence_output, pooled_output = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), \n","                                                   attention_mask = attention_mask.float().to(token_ids.device))\n","        return sequence_output, pooled_output\n","        #sequence_output:(nums of sentences, hidden_size) #pooled_output:(nums of sentences, max_len, hidden_size)\n","        # 디코더 인풋으로 들어갈 수 있게 형식 맞춰주는 후작업 필요"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"3S7CE_RM6RjT","executionInfo":{"status":"ok","timestamp":1604595684909,"user_tz":-540,"elapsed":2998,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}},"outputId":"e1c0216e-1bbd-49e2-dd2b-c3def8805ed8","colab":{"base_uri":"https://localhost:8080/"}},"source":["input_text = input(\"텍스트를 입력하세요: \")"],"execution_count":77,"outputs":[{"output_type":"stream","text":["텍스트를 입력하세요: 안녕하세요. 저는 투빅스 13기 고유경입니다. 중간고사가 방금 끝나서 피곤해요. 내일은 논문을 읽어야 해요. 인생이란 어려우면서도 배울게 많은 여정이랄까요. 여러 생각이 스치는 새벽입니다. 다들 좋은 밤 되시길 바라요.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9JmIELdzGhB8","executionInfo":{"status":"ok","timestamp":1604595687222,"user_tz":-540,"elapsed":852,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}}},"source":["# kobert는 데이터셋 불러오는 작업이 특이해서 input text를 txt 파일로 변환 후 다시 로딩해야합니다.\n","# 그래서 위에 dataset 클래스가 새로 정의되어있는거고, 아예 BERTDataset 모듈도 따로 있어요\n","\n","input_text = kss.split_sentences(input_text) # -> list of sentences\n","essay = pd.DataFrame(input_text) # -> dataframe\n","essay['label'] = 1  #의미없음 #이거 안해주면 df 저장 안됨\n","time_info = datetime.datetime.now().strftime('%Y%m%d%H%M%S') # 파일 이름 설정용 현재시간 로드\n","save_link = \"sample_data/{}.txt\".format(time_info) # 현재 위치 폴더에 맞게 경로 재설정해야함\n","essay.to_csv(save_link, sep='\\t', index_label='idx') # csv파일로 저장\n","#input text data 불러오기\n","dataset_sentences = nlp.data.TSVDataset(save_link, field_indices=[1, 2], num_discard_samples=1)\n","data_sentences = BERTDataset(dataset_sentences, 0, 1, tok, 30, True, False) # max_len (30) #예시문장이 다 짧아서 30으로 지정 #자유롭게 바꿔도됨\n","sentences_dataloader = torch.utils.data.DataLoader(data_sentences, batch_size=len(data_sentences), num_workers=5)"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"gN0dXMTXZgCx","executionInfo":{"status":"ok","timestamp":1604595692256,"user_tz":-540,"elapsed":1640,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}}},"source":["encoder = EncoderBERT(bertmodel).to(device) #위에서 만든 encoder 로드\n","encoder.eval()\n","\n","with torch.no_grad(): #파라미터 고정 #만약에 파라미터 학습시키려면 따로 코드를 더 추가해야돼서 연락주세용\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(sentences_dataloader):\n","            token_ids = token_ids.long().to(device)\n","            segment_ids = segment_ids.long().to(device)\n","            label = label.long().to(device)\n","            valid_length = valid_length\n","            sequence_output, pooled_output = encoder(token_ids, valid_length, segment_ids) #최종 인코더 아웃풋 도출"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"r420H4sQTWt4","executionInfo":{"status":"ok","timestamp":1604595711372,"user_tz":-540,"elapsed":819,"user":{"displayName":"‍고유경[ 학부재학 / 미디어학부 ]","photoUrl":"","userId":"00276969175911314601"}},"outputId":"c1b2f4a6-168e-4e27-e5a8-ebaa032fe8c0","colab":{"base_uri":"https://localhost:8080/"}},"source":["# 혼란스러울 때, 하나하나 파악하기 위해 찍어보는 코드~\n","print(\"1. input_ids: \", token_ids.shape, \"\\n\", token_ids)\n","print(\"---------------------\")\n","print(\"2. segment_ids: \", segment_ids.shape, \"\\n\", segment_ids.long())\n","print(\"---------------------\")\n","print(\"3. 인코더 결과 1) sequence_output: \", sequence_output.shape, \"\\n\", sequence_output)\n","print(\"---------------------\")\n","print(\"4. 인코더 결과 2) pooled_output: \", pooled_output.shape, \"\\n\", pooled_output)"],"execution_count":81,"outputs":[{"output_type":"stream","text":["1. input_ids:  torch.Size([7, 30]) \n"," tensor([[   2, 3135, 5724, 7814,  517,   54,    3,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1],\n","        [   2, 3990, 5760, 4762, 6449, 6664,  541, 5561,  993, 7063, 5424, 7139,\n","          517,   54,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1],\n","        [   2, 4258, 5439, 6494, 2267, 5550, 1365, 6553, 4909, 5444, 7848, 6999,\n","          517,   54,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1],\n","        [   2, 1434, 7126, 7086, 1498, 6235, 3824, 6856, 4998, 6999,  517,   54,\n","            3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1],\n","        [   2, 3774, 7107, 3220, 6060, 7005, 6200, 2287, 7013, 5400, 1955, 3298,\n","         7227, 7096, 6017, 5591, 6999,  517,   54,    3,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1],\n","        [   2, 3304, 2707, 2929, 7485, 2701, 7139,  517,   54,    3,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1],\n","        [   2, 1562, 5931, 4209, 2265, 1763, 6705, 5585, 2193, 6999,  517,   54,\n","            3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1]])\n","---------------------\n","2. segment_ids:  torch.Size([7, 30]) \n"," tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0]])\n","---------------------\n","3. 인코더 결과 1) sequence_output:  torch.Size([7, 30, 768]) \n"," tensor([[[ 0.0543, -0.3337,  0.1722,  ...,  0.2000,  0.0928, -0.3034],\n","         [-0.3035,  0.2061, -0.1855,  ...,  0.3150,  0.4011, -0.1111],\n","         [-0.0878, -0.4898, -0.0702,  ...,  0.0367,  0.3556, -0.3637],\n","         ...,\n","         [ 0.3581, -0.4208,  0.2145,  ..., -0.0179, -0.4965, -0.1454],\n","         [ 0.3581, -0.4208,  0.2145,  ..., -0.0179, -0.4965, -0.1454],\n","         [ 0.3581, -0.4208,  0.2145,  ..., -0.0179, -0.4965, -0.1454]],\n","\n","        [[-0.5158,  0.1588, -0.0182,  ..., -0.0336, -0.1508, -0.0557],\n","         [ 0.0408,  0.0154, -0.2642,  ...,  0.0320,  0.1685, -0.1957],\n","         [ 0.4210, -0.2656,  0.0672,  ..., -0.2615, -0.6335, -0.2873],\n","         ...,\n","         [-0.1163, -0.3714,  0.1253,  ..., -0.2487, -0.4208, -0.0439],\n","         [-0.1163, -0.3714,  0.1253,  ..., -0.2487, -0.4208, -0.0439],\n","         [-0.1163, -0.3714,  0.1253,  ..., -0.2487, -0.4208, -0.0439]],\n","\n","        [[-0.0696, -0.5916, -0.0591,  ..., -0.0619,  0.1507, -0.3516],\n","         [ 0.0252,  0.0182,  0.0799,  ...,  0.3097,  0.2144, -0.1011],\n","         [ 0.4524, -0.2296,  0.1969,  ...,  0.2619,  0.1348, -0.6403],\n","         ...,\n","         [ 0.3159, -0.4891,  0.0226,  ..., -0.1532, -0.0809, -0.2836],\n","         [ 0.3159, -0.4891,  0.0226,  ..., -0.1532, -0.0809, -0.2836],\n","         [ 0.3159, -0.4891,  0.0226,  ..., -0.1532, -0.0809, -0.2836]],\n","\n","        ...,\n","\n","        [[ 0.1210, -0.0864,  0.1678,  ..., -0.0968,  0.0558, -0.1650],\n","         [ 0.0512,  0.1952,  0.3922,  ...,  0.3275, -0.0683,  0.0846],\n","         [ 0.4197,  0.2148,  0.3220,  ..., -0.0904,  0.1378, -0.5744],\n","         ...,\n","         [ 0.2574, -0.3382,  0.0197,  ..., -0.3562, -0.1765, -0.2619],\n","         [ 0.2574, -0.3382,  0.0197,  ..., -0.3562, -0.1765, -0.2619],\n","         [ 0.2574, -0.3382,  0.0197,  ..., -0.3562, -0.1765, -0.2619]],\n","\n","        [[-0.2626, -0.4837, -0.1924,  ...,  0.0466, -0.2365, -0.1155],\n","         [-0.1055, -0.1391, -0.2349,  ..., -0.1306,  0.0612,  0.4041],\n","         [-0.0541, -0.3400, -0.4345,  ...,  0.0651,  0.4143, -0.0803],\n","         ...,\n","         [-0.0101, -0.2443, -0.4001,  ...,  0.1744, -0.2612, -0.3065],\n","         [-0.0101, -0.2443, -0.4001,  ...,  0.1744, -0.2612, -0.3065],\n","         [-0.0101, -0.2443, -0.4001,  ...,  0.1744, -0.2612, -0.3065]],\n","\n","        [[ 0.1776, -0.3098,  0.5334,  ..., -0.0211, -0.2419, -0.3960],\n","         [ 0.3651, -0.2396,  0.2858,  ..., -0.4960,  0.4460, -0.1783],\n","         [ 0.7280, -0.4800,  0.2107,  ..., -0.3289,  0.0404, -0.2908],\n","         ...,\n","         [ 0.6096, -0.5127,  0.6903,  ..., -0.3820, -0.5654, -0.2384],\n","         [ 0.6096, -0.5127,  0.6903,  ..., -0.3820, -0.5654, -0.2384],\n","         [ 0.6096, -0.5127,  0.6903,  ..., -0.3820, -0.5654, -0.2384]]])\n","---------------------\n","4. 인코더 결과 2) pooled_output:  torch.Size([7, 768]) \n"," tensor([[-0.0637, -0.0119, -0.1241,  ...,  0.0673, -0.0415,  0.0400],\n","        [-0.0993,  0.0504, -0.3199,  ...,  0.0970, -0.0052,  0.0752],\n","        [-0.0505,  0.0201,  0.3868,  ...,  0.0802, -0.0269, -0.0337],\n","        ...,\n","        [-0.0285,  0.0038, -0.3851,  ...,  0.0085,  0.0049,  0.0301],\n","        [-0.0230,  0.0188,  0.2098,  ...,  0.0928, -0.0475,  0.0131],\n","        [-0.0263, -0.0318,  0.2990,  ...,  0.0881, -0.0344, -0.0643]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bbFDCVmUIa8m"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"-Sbo2w0gTAsp"},"source":["버트가 좀 복잡하죵.. 코드도 너저분하공.... 그래도 화이팅해봅시당!!"]}]}